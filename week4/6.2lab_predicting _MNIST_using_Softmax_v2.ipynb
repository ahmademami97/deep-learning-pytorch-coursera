{"cells":[{"cell_type":"markdown","metadata":{},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/IDSNlogo.png\" width=\"300\" alt=\"cognitiveclass.ai logo\"  />\n","</center>\n"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Softmax Classifier</h1>\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Objective</h2><ul><li> How to classify handwritten digits from the MNIST database by using Softmax classifier.</li></ul> \n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Table of Contents</h2>\n","<p>In this lab, you will use a single layer Softmax to classify handwritten digits from the MNIST database.</p>\n","\n","<ul>\n","    <li><a href=\"#Makeup_Data\">Make some Data</a></li>\n","    <li><a href=\"#Classifier\">Softmax Classifier</a></li>\n","    <li><a href=\"#Model\">Define Softmax, Criterion Function, Optimizer, and Train the Model</a></li>\n","    <li><a href=\"#Result\">Analyze Results</a></li>\n","</ul>\n","<p>Estimated Time Needed: <strong>25 min</strong></p>\n","\n","<hr>\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>Preparation</h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["We'll need the following libraries\n"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Import the libraries we need for this lab\n","\n","# Using the following line code to install the torchvision library\n","# !conda install -y torchvision\n","\n","#!pip install torchvision==0.9.1 torch==1.8.1 \n","import os    \n","os.environ['KMP_DUPLICATE_LIB_OK']='True'\n","import torch \n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","import torchvision.datasets as dsets\n","import matplotlib.pylab as plt\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["Use the following function to plot out the parameters of the Softmax function:\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# The function to plot parameters\n","\n","def PlotParameters(model): \n","    W = model.state_dict()['linear.weight'].data\n","    w_min = W.min().item()\n","    w_max = W.max().item()\n","    fig, axes = plt.subplots(2, 5)\n","    fig.subplots_adjust(hspace=0.01, wspace=0.1)\n","    for i, ax in enumerate(axes.flat):\n","        if i < 10:\n","            \n","            # Set the label for the sub-plot.\n","            ax.set_xlabel(\"class: {0}\".format(i))\n","\n","            # Plot the image.\n","            ax.imshow(W[i, :].view(28, 28), vmin=w_min, vmax=w_max, cmap='seismic')\n","\n","            ax.set_xticks([])\n","            ax.set_yticks([])\n","\n","        # Ensure the plot is shown correctly with multiple plots\n","        # in a single Notebook cell.\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["Use the following function to visualize the data: \n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["# Plot the data\n","\n","def show_data(data_sample):\n","    plt.imshow(data_sample[0].numpy().reshape(28, 28), cmap='gray')\n","    plt.title('y = ' + str(data_sample[1]))"]},{"cell_type":"markdown","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"Makeup_Data\">Make Some Data</h2> \n"]},{"cell_type":"markdown","metadata":{},"source":["Load the training dataset by setting the parameters <code>train</code> to <code>True</code> and convert it to a tensor by placing a transform object in the argument <code>transform</code>.\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["9913344it [00:12, 803831.57it/s]                             \n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["29696it [00:00, 1751544.09it/s]          \n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["1649664it [00:02, 766583.21it/s]                             \n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n","\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["5120it [00:00, ?it/s]                   \n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n","\n","Print the training dataset:\n","  Dataset MNIST\n","    Number of datapoints: 60000\n","    Root location: ./data\n","    Split: Train\n","    StandardTransform\n","Transform: ToTensor()\n"]}],"source":["# Create and print the training dataset\n","\n","train_dataset = dsets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n","print(\"Print the training dataset:\\n \", train_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Load the testing dataset and convert it to a tensor by placing a transform object in the argument <code>transform</code>.\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Print the validating dataset:\n","  Dataset MNIST\n","    Number of datapoints: 60000\n","    Root location: ./data\n","    Split: Train\n","    StandardTransform\n","Transform: ToTensor()\n"]}],"source":["# Create and print the validating dataset\n","\n","validation_dataset = dsets.MNIST(root='./data', download=True, transform=transforms.ToTensor())\n","print(\"Print the validating dataset:\\n \", validation_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["You can see that the data type is long:\n"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Type of data element:  <class 'int'>\n"]}],"source":["# Print the type of the element\n","\n","print(\"Type of data element: \", type(train_dataset[0][1]))"]},{"cell_type":"markdown","metadata":{},"source":["Each element in the rectangular tensor corresponds to a number that represents a pixel intensity as demonstrated by the following image:\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.32_image_values.png\" width=\"550\" alt=\"MNIST elements\" />\n"]},{"cell_type":"markdown","metadata":{},"source":["In this image, the values are inverted i.e back represents wight.\n"]},{"cell_type":"markdown","metadata":{},"source":["Print out the label of the fourth element:\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The label:  1\n"]}],"source":["# Print the label\n","\n","print(\"The label: \", train_dataset[3][1])"]},{"cell_type":"markdown","metadata":{},"source":["The result shows the number in the image is 1\n"]},{"cell_type":"markdown","metadata":{},"source":["Plot  the fourth sample:\n"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The image:  None\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Plot the image\n","\n","print(\"The image: \", show_data(train_dataset[3]))"]},{"cell_type":"markdown","metadata":{},"source":["You see that it is a 1. Now, plot the third sample:\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Plot the image\n","\n","show_data(train_dataset[2])"]},{"cell_type":"markdown","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"#Classifier\">Build a Softmax Classifer</h2>\n"]},{"cell_type":"markdown","metadata":{},"source":["Build a Softmax classifier class: \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define softmax classifier class\n","\n","class SoftMax(nn.Module):\n","    \n","    # Constructor\n","    def __init__(self, input_size, output_size):\n","        super().__init__()\n","        self.linear = nn.Linear(input_size, output_size)\n","        \n","    # Prediction\n","    def forward(self, x):\n","        z = self.linear(x)\n","        return z"]},{"cell_type":"markdown","metadata":{},"source":["The Softmax function requires vector inputs. Note that the vector shape is 28x28.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the shape of train dataset\n","\n","train_dataset[0][0].shape"]},{"cell_type":"markdown","metadata":{},"source":["Flatten the tensor as shown in this image: \n"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.3.2image_to_vector.gif\" width=\"550\" alt=\"Flattern Image\" />\n"]},{"cell_type":"markdown","metadata":{},"source":["The size of the tensor is now 784.\n"]},{"cell_type":"markdown","metadata":{},"source":["<img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.3.2Imagetovector2.png\" width=\"550\" alt=\"Flattern Image\" />\n"]},{"cell_type":"markdown","metadata":{},"source":["Set the input size and output size: \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set input size and output size\n","\n","input_dim = 28 * 28\n","output_dim = 10"]},{"cell_type":"markdown","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"Model\">Define the Softmax Classifier, Criterion Function, Optimizer, and Train the Model</h2> \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create the model\n","\n","model = SoftMax(input_dim, output_dim)\n","print(\"Print the model:\\n \", model)"]},{"cell_type":"markdown","metadata":{},"source":["View the size of the model parameters: \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print the parameters\n","\n","print('W: ',list(model.parameters())[0].size())\n","print('b: ',list(model.parameters())[1].size())"]},{"cell_type":"markdown","metadata":{},"source":["You can cover the model parameters for each class to a rectangular grid:  \n"]},{"cell_type":"markdown","metadata":{},"source":["<a>     <img src = \"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0110EN/notebook_images%20/chapter3/3.3.2paramaters_to_image.gif\" width = 550, align = \"center\"></a> \n"]},{"cell_type":"markdown","metadata":{},"source":["Plot the model parameters for each class as a square image: \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the model parameters for each class\n","\n","PlotParameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["Define the learning rate, optimizer, criterion, data loader:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define the learning rate, optimizer, criterion and data loader\n","\n","learning_rate = 0.1\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n","criterion = nn.CrossEntropyLoss()\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n","validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=5000)"]},{"cell_type":"markdown","metadata":{},"source":["Train the model and determine validation accuracy **(should take a few minutes)**: \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Train the model\n","\n","n_epochs = 10\n","loss_list = []\n","accuracy_list = []\n","N_test = len(validation_dataset)\n","\n","def train_model(n_epochs):\n","    for epoch in range(n_epochs):\n","        for x, y in train_loader:\n","            optimizer.zero_grad()\n","            z = model(x.view(-1, 28 * 28))\n","            loss = criterion(z, y)\n","            loss.backward()\n","            optimizer.step()\n","            \n","        correct = 0\n","        # perform a prediction on the validationdata  \n","        for x_test, y_test in validation_loader:\n","            z = model(x_test.view(-1, 28 * 28))\n","            _, yhat = torch.max(z.data, 1)\n","            correct += (yhat == y_test).sum().item()\n","        accuracy = correct / N_test\n","        loss_list.append(loss.data)\n","        accuracy_list.append(accuracy)\n","\n","train_model(n_epochs)"]},{"cell_type":"markdown","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2 id=\"Result\">Analyze Results</h2> \n"]},{"cell_type":"markdown","metadata":{},"source":["Plot the loss and accuracy on the validation data:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the loss and accuracy\n","\n","fig, ax1 = plt.subplots()\n","color = 'tab:red'\n","ax1.plot(loss_list,color=color)\n","ax1.set_xlabel('epoch',color=color)\n","ax1.set_ylabel('total loss',color=color)\n","ax1.tick_params(axis='y', color=color)\n","    \n","ax2 = ax1.twinx()  \n","color = 'tab:blue'\n","ax2.set_ylabel('accuracy', color=color)  \n","ax2.plot( accuracy_list, color=color)\n","ax2.tick_params(axis='y', color=color)\n","fig.tight_layout()"]},{"cell_type":"markdown","metadata":{},"source":["View the results of the parameters for each class after the training. You can see that they look like the corresponding numbers. \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the parameters\n","\n","PlotParameters(model)"]},{"cell_type":"markdown","metadata":{},"source":["We Plot the first five misclassified  samples and the probability of that class.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the misclassified samples\n","Softmax_fn=nn.Softmax(dim=-1)\n","count = 0\n","for x, y in validation_dataset:\n","    z = model(x.reshape(-1, 28 * 28))\n","    _, yhat = torch.max(z, 1)\n","    if yhat != y:\n","        show_data((x, y))\n","        plt.show()\n","        print(\"yhat:\", yhat)\n","        print(\"probability of class \", torch.max(Softmax_fn(z)).item())\n","        count += 1\n","    if count >= 5:\n","        break       "]},{"cell_type":"markdown","metadata":{},"source":["<!--Empty Space for separating topics-->\n"]},{"cell_type":"markdown","metadata":{},"source":["We Plot the first five correctly classified samples and the probability of that class, we see the probability is much larger.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the classified samples\n","Softmax_fn=nn.Softmax(dim=-1)\n","count = 0\n","for x, y in validation_dataset:\n","    z = model(x.reshape(-1, 28 * 28))\n","    _, yhat = torch.max(z, 1)\n","    if yhat == y:\n","        show_data((x, y))\n","        plt.show()\n","        print(\"yhat:\", yhat)\n","        print(\"probability of class \", torch.max(Softmax_fn(z)).item())\n","        count += 1\n","    if count >= 5:\n","        break  "]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"https://dataplatform.cloud.ibm.com/registration/stepone?context=cpdaas&apps=data_science_experience,watson_machine_learning\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0110EN-SkillsNetwork/Template/module%201/images/Watson_Studio.png\"/></a>\n"]},{"cell_type":"markdown","metadata":{},"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD. \n"]},{"cell_type":"markdown","metadata":{},"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>\n"]},{"cell_type":"markdown","metadata":{},"source":["## Change Log\n","\n","| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n","| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n","| 2020-09-23        | 2.0     | Shubham    | Migrated Lab to Markdown and added to course repo in GitLab |\n"]},{"cell_type":"markdown","metadata":{},"source":["<hr>\n"]},{"cell_type":"markdown","metadata":{},"source":["## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"]}],"metadata":{"interpreter":{"hash":"cc5f70855ac006f3de45a3cc3b9e7d8d53845e50458809cb162b0174266dec97"},"kernelspec":{"display_name":"Python","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":4}
